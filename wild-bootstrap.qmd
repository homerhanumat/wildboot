---
title: "Wild Bootstrap Confidence Intervals"
subtitle: "Response to queries from Sara Incera"
format:
  lumo-html: 
    github-repo: "https://www.github.com/homerhanumat/wildboot"
    primary-color: "#a31f37"
    self-contained: true
    is-particlejs-enabled: true
author: Homer White
date: last-modified
---



## Questions

You had these three questions:

1. Do I need to change the `bootDIstn = "normal"` option to something else?
1. Can I run this bootstrap method if the distribution of the residuals is bimodal?
1. How can I get the confidence intervals for these five estimates? 

My answers:

1. No.  The options for `bootDIstn` implement various attempts to get slightly more powerful results at small sample sizes in specific situations. They do not correspond to the observed distribution of the residuals. 
1. Yes.  The bootstrap is designed to handle situations where one cannot make assumptions about the distribution of the residuals.
1. See the rest of this note.

But first let's attach some packages:

```{r setup}
#| warning: false
#| message: false
#| code-fold: false
library(tidyverse)
library(lmboot)
library(DT)
library(knitr)
library(readxl)
```

## A Small Example

From the `mtcars` data table in the **dataset** package:

```{r}
#| code-fold: false
mtcars2 <-
  mtcars %>% 
  mutate(
    vs = factor(vs), 
    am = factor(am)
  ) %>% 
  mutate(
    vs = fct_recode(vs, "V-shaped" = "0", "straight" = "1"),
    am = fct_recode(am, "automatic" = "0", "manual" = "1")
  ) %>% 
  select(mpg, disp, wt, vs, am)
```

Here's the example table:


```{r}
#| tbl-cap: "Motor Trend cars"
DT::datatable(
  mtcars2
)
```



## Regression

Let's predict mileage from the other variables:

```{r}
#| code-fold: false
mod <- wild.boot(
  mpg ~ .,
  B = 4999,
  data = mtcars2,
  seed = 4949, ## for reproducibility,
  bootDistn = "normal"
)
```


The bootstrap re-samples are given in a matrix (`mod$bootEstParam`) where the columns are named:

```{r}
#| tbl-cap: "Bootstrap re-samples"
resamples <-
  mod$bootEstParam

DT::datatable(
  resamples %>% as.data.frame() %>% round(3)
)
```

## Point estimates for the Parameters

The point estimate for a parameter is the mean of the re-samples for that parameter.  You have indicated you know how to access them:

```{r}
#| tbl-cap: "Parameter point-estimates"
mod$origEstParam %>% 
  knitr::kable()
```


## Confidence Intervals

You are left to your own devices to construct a confidence interval for a given model parameter, using the re-samples in provided in the column associated with that parameter. That's because there are many ways to make the interval.

One simple option---perfectly acceptable here, I think---is the so-called "percentile" interval.

For example, to make an approximate 95%-confidence interval for your parameter, just take the 2.5th and 97.5th percentiles of your data.

Here's a little function to compute intervals of any desired level of confidence, for any desired parameter, starting from a wild model:

```{r}
#| code-fold: false
conf_int <- function(model, parameter, level = 0.95) {
  resample_matrix <- model$bootEstParam
  resamples <- resample_matrix[, parameter]
  bounds <- quantile(resamples, probs = c((1 - level) / 2, (1 + level) / 2))
  names(bounds) <- c("lower", "upper")
  bounds
}
```

Let's try it out.  Here is a 90%-confidence interval for `ammanual` (the extra miles-per-gallon fuel efficiency of automatic over manual transmission):

```{r}
#| code-fold: false
interval <-
  conf_int(
    model = mod,
    parameter = "ammanual",
    level = 0.90
  )

interval
```

So we estimate that the benefit of automatic over manual is about `r round(mod$origEstParam[5,1], 3)` miles per gallon, and we are 90%-percent confident that it is somewhere between `r round(interval[1], 3)` and `r round(interval[2], 3)` miles per gallon.

You might want to put all of your confidence intervals in one place.  Here's one approach:

```{r}
#| code-fold: false
make_intervals <- function(mod, level = 0.95) {
  resamples <- mod$bootEstParam
  parameters <- colnames(resamples)
  parameters %>% 
    purrr::map_dfr(function(param) {
      interval <- conf_int(
        model = mod,
        level = level,
        parameter = param
      )
      list(
        parameter = param,
        `lower bound` = interval[1],
        `upper bound` = interval[2]
      )
    })
}

all_intervals <- make_intervals(mod = mod)

knitr::kable(
  all_intervals,
  caption = "95%-confidence intervals for parameters of the linear model"
)
```


## Thoughts on the Reviewer's Comments

The reviewer said:

>Furthermore, the authors should check whether the data is normally distributed. If the data normality assumption has not been met, they should choose to conduct multiple regression analyses using a non-parametric method, such as a bootstrap method (e.g., percentile bootstrap). Given the number of independent and dependent variables in the regression analyses, it is more reliable to use bootstrap confidence intervals instead of p values to determine whether the independent variables significantly predict the dependent variables.

### P-Values

I would note that there are ways to compute P-values with the bootstrap, so the bootstrap is often used for hypothesis testing.  (Making confidence intervals rather than computing P-values doesn't render one's analysis more "reliable".)

In our study, we might be planning four two-sided tests of hypothesis, one for each of the four predictor variables, each having the form:

* $H_0$:  $\theta = 0$ (all the other predictors held equal, this predictor is not associated with the response variable)
* $H_a$:  $\theta \neq 0$ (it *is* thus associated)

One way to compute a bootstrap confidence interval is by *inversion*, which makes use of these facts about the relationship between confidence-intervals and testing:

* The p-value of the two-sided test for the parameter $\theta$ is the smallest number $\alpha$ such that null value $\theta_0$---the value assigned to $\theta$ by the Null Hypothesis---does not lie in the $1-\alpha$-confidence interval for $\theta$.
* In two-sided testing of the parameter $\theta$ with significance-level $\alpha$, the set of null values $\theta_0$ that would not be rejected in the test forms a $1-\alpha$-confidence interval for $\theta$.

Here is a function to compute a two-sided bootstrap p-value:

```{r}
boot_p_val <- function(model, param, theta) {
  resamples <- model$bootEstParam[, param]
  n <- length(resamples)
  sorted <- sort(resamples)
  if (theta < min(resamples) | theta > max(resamples)) return(1 / (n + 1))
  theta_rank <- min(
    min(which(theta <= sorted)),
    n - max(which(theta >= sorted))
  )
  theta_perc <- theta_rank / (n + 1)
  2 * theta_perc
}
```

Let's make a new table that shows the p-values for the predictors.  (We'll leave out the intercept parameter because we are not interested in whether or not it has the value 0.)


```{r}
make_table <- function(mod, level = 0.95) {
  resamples <- mod$bootEstParam[, -1]
  parameters <- colnames(resamples)
  parameters %>% 
    purrr::map_dfr(function(param) {
      interval <- conf_int(
        model = mod,
        level = level,
        parameter = param
      )
      list(
        parameter = param,
        `lower bound` = interval[1],
        `upper bound` = interval[2],
        `p-value` = boot_p_val(model = mod, param = param, theta = 0)
      )
    })
}

all_params <- make_table(mod = mod)

knitr::kable(
  all_params,
  caption = "Parameter estimates with 95%-confidence intervals and p-values"
)
```

### What's the True Rationale for Using the Bootstrap?

We should also be quite clear that the need for the bootstrap derives soley from the failure of the data to meet all of the assumptions of the standard linear model (including normality of residuals).  In particular, the number of independent or dependent variables has no bearing on whether to use the parametric or non-parametric methods of inferential analysis.

### The Problem of Multiple Analyses

On the other hand, the number of dependent and independent variables *can* affect the number of research questions you are asking.  i'm sure you are well-aware of recent concerns in the social sciences about findings that don't replicate, and you know that it would be bad practice to to look at a lot of intervals and make a big deal about the ones that don't contain 0 (i.e., show "significant" results).  If one were in a situation where nothing relates to anything (say, the data is just random, so that true value of all parameters in the models are 0) and were to construct 95%-confidence intervals for $N$ different parameters, then there would be a

$$1-0.95^N$$

chance that at least one of the intervals will not contain 0, corresponding to a P-value of less than 0.05 in a two-sided test of hypothesis and thereby signalling (to a careless analyst) "publication-worthy evidence" of a relationship between the corresponding independent variable and the dependent variable in the model---even though there is in reality *no such relationship*.  The larger $N$ is, the greater the chance of such false positives.

In your study, I'm seeing 5 independent variable and 18 dependent variables, resulting in

$$N = 5 \times 18 = 90$$

confidence intervals.  If all variables are actually independent of one another, then the chance of at least one "significant" result indicating dependence is:

```{r}
#| code-fold: false
1 - 0.95^90
```

That's nearly a 100% chance.  Also, one would expect to observe about

$$0.05 \times 90 = 4.5$$

of the confidence intervals to fail to contain 0 (thus indicating "statistical significance at the 5%-level") even though there are no relationships anywhere.

:::{.callout-important}
* Using the bootstrap does nothing at all to address the problem of multiple tests.
* Making confidence intervals instead of performing tests of hypothesis does nothing at all to address the problem of multiple tests.
:::




::: {.column-margin}

I hope the reviewer did not think that bootstrap confidence intervals would address this problem.  (I have come across bad advice on the web to this effect.)

:::


### Correcting for Multiple Analyses

If one does plan to perform multiple analyses, one should adopt a method that controls for the proliferation of false-positive results.  A number of control methods are available.  One approach that is rather "conservative" but that applies in all situations of multiple analysis is the [Holm-Bonferroni](https://en.wikipedia.org/wiki/Holm%E2%80%93Bonferroni_method) method, which works as follows:

1. Let $\alpha$ be the nominal desired level of significance (commonly $\alpha = 0.05$).
2. Given $N$ P-values, arrange them in order from the lowest to highest as:  $p_1, p_2, \ldots, p_N$.
3. Set the variable $m$ to 1.
4. Check whether $p_{m} \leq \alpha/(N-m+1)$.  If this is not the case, then stop.  If it does happen, then proceed to the next step.
5. Increase $m$ by 1, and return to Step 4 above.

When the above process stops, the values $p_1, \ldots, p_{m}$ are declared significant.  (Note that there may be integers $n > m$ where $p_n \leq \alpha / (N-n+1)$, but the corresponding P-values won't be declared significant.)

We implement Holm-Bonferroni here:

```{r}
#| code-fold: false
holm_bon <- function(pvals, alpha = 0.05) {
  df <- 
    data.frame(
      id = 1:length(pvals),
      pval = pvals,
      m = order(pvals)
    ) %>% 
    arrange(pval) %>% 
    mutate(
      divisor = length(pvals):1,
      holmbon = alpha / divisor
    ) %>% 
    mutate(pass = pval <= holmbon) %>% 
    mutate(c = cumsum(pass)) %>% 
    mutate(significant = c >= 1:length(pvals)) %>% 
    arrange(id) %>% 
    select(-pass)
  df
}
```

Let's try it on four p-values, with significance-level $\alpha = 0.05$.

```{r}
holm_bon(c(0.02, 0.01, 0.021, 0.11)) %>% 
  rename(`N - m + 1` = divisor) %>% 
  select(-id) %>% 
  knitr::kable(
    caption = "Holm-Bonferroni applied to a few hypothetical p-values (alpha = 0.05)"
  )
```

(**Note**:  In the above table, `holmbon` gives the value of $\alpha / (N-m+1)$ to which $p_m$ would be compared in the Holm-Bonferroni process.  We see that the p-value 0.021 is less than the `holmbon` value, but it is not counted as significant because the Holm-bonferroni process stopped at the smaller p-value 0.02.)

An important property of the Holm-Bonferroni criterion for rejection is that if all of the null-hypotheses are actually true, then the probability of falsely rejecting at least one of them is no more than $\alpha$.

Let's incorporate Holm-Bonferroni in our analysis:

```{r}
make_table <- function(mod, level = 0.95) {
  resamples <- mod$bootEstParam[, -1]
  parameters <- colnames(resamples)
  df <-
    parameters %>% 
    purrr::map_dfr(function(param) {
      interval <- conf_int(
        model = mod,
        level = level,
        parameter = param
      )
      list(
        parameter = param,
        `lower bound` = interval[1],
        `upper bound` = interval[2],
        `p-value` = boot_p_val(model = mod, param = param, theta = 0)
      )
    })
  pvals <- df$`p-value`
  holmbon_df <- holm_bon(pvals = pvals, alpha = 1 - level)
  df2 <- 
    df %>% 
    mutate(
      `Holm-Bonferroni Level` = holmbon_df$holmbon,
      `significant` = holmbon_df$significant
    )
  df2
}

all_params <- make_table(mod = mod)

knitr::kable(
  all_params,
  caption = "Parameter estimates with 95%-confidence intervals, uncorrected p-values and Holm-Bonferroni rejection-criteria"
)
```

(**Note**:  In the table above, `Holm-Bonferroni Level` gives the value of $\alpha / (N-m+1)$.)

Of course, if you had planned in your study to perform all 90 tests, then you will have to combine the P-values from the 18 models into one vector of length 90 in order to apply the Holm-Bonferooni procedure.  You can't apply Holm-Bonferri separately to each dependent variable study:  that would be like pretending that you are doing only one study instead of 18.

Let's do this for the data you have on hand.  First, we'll read in and process the data:


```{r}
Dataset1 <- read_excel("data/Data1.xlsx")
Dataset1$Sex <- as.factor(Dataset1$Sex)
DatasetClean <- drop_na(Dataset1)
```

Here are the predictor variables and the dependent variables:


```{r}
predictors <-
  c(
    "Sex",
    "Independent",
    "Interdependent",
    "Relational",
    "Physical"
  )
dependents <-
  c(
    "CreativeArts",
    "PerformingArts",
    "Law",
    "Politics",
    "TechnicalWriting",
    "InformationTech",
    "Mathematics",
    "Management",
    "OfficeWork",
    "Business",
    "Finance",
    "Sales",
    "FamilyActivity",
    "SocialServices",
    "OutdoorAgriculture",
    "ManualLabor",
    "Engineering",
    "ProtectiveServices"
    )
```

Here is a function to perform the analysis for a single dependent variable:


```{r}
## helper function:
make_one_table <- function(mod, dep, level = 0.95) {
  resamples <- mod$bootEstParam[, -1]
  parameters <- colnames(resamples)
  df <-
    parameters %>% 
    purrr::map_dfr(function(param) {
      interval <- conf_int(
        model = mod,
        level = level,
        parameter = param
      )
      list(
        dep = dep,
        parameter = param,
        `lower bound` = interval[1],
        `upper bound` = interval[2],
        `p-value` = boot_p_val(model = mod, param = param, theta = 0)
      )
    })
  df
}

## function to do the study for one dependent variable:
one_dependent <- function(dep, preds, data, level = 0.95) {
  formula_elements <-
    c(
      dep,
      "  ~ ",
      paste0(preds[1:(length(preds) - 1)], sep = " + "),
      " ",
      preds[length(preds)]
    )
  formula_string <- paste0(formula_elements, collapse = "")
  formula <- as.formula(formula_string)
  mod <- wild.boot(
    formula = formula,
    data = data,
    bootDistn = "normal",
    B = 4999
  )
  make_one_table(mod = mod, dep = dep, level = level)
}

tab <- one_dependent(
  dep = "Mathematics", 
  preds = predictors, 
  data = DatasetClean
)
```

Now let's do all the studies, compuing confidence intervals and P-values, and implementing Holm-Bonferroni:

```{r}
set.seed(5050)
all_tabs <-
  dependents %>% 
  map_dfr(one_dependent, preds = predictors, data = DatasetClean)

holm <- holm_bon(all_tabs$`p-value`)
all_tabs_complete <-
  all_tabs %>% 
  mutate(
      `Holm-Bonferroni Level` = holm$holmbon,
      `significant` = holm$significant
    )
```

Here are the results:

::: {.column-page-inset-right}

```{r}

DT::datatable(
  all_tabs_complete %>% 
    mutate(
      mutate(across(where(is.numeric), function(x) round(x, 5)))
    )
)
```

:::

For convenient reference, below are the analyses that were deemed statistically significant at the $\alpha = 0.05$ level:

```{r}
knitr::kable(
  all_tabs_complete %>% 
    filter(significant) %>% 
    select(
      dep, parameter, `p-value`, `Holm-Bonferroni Level`
    ),
  caption = "Significant predictors (alpha = 0.05 level)"
)
```


### Alternative Approaches to Multiple Analyses

I mentioned that the Holm-Bonferroni method is rather "conservative", in that it guarantees that when all of the null-hypotheses are actually true, then the probability of falsely rejecting at least one of them is no more than $\alpha$. In many applications this probability is liable to be considerably less than $\alpha$, resulting in substantially less power to detect true alternative hypotheses.

Other classical methods, such as those of Hochberg or Hommel, have been demonstrated to be uniformly more powerful than Holm-Bonferroni, but they are applicable only under particular conditions concerning the dependence between variables under study, which conditions are unlikely to hold in your case.

In response to the widespread accessibility, in recent decades, of high-speed computation. statisticians have developed computationally-intensive methods for correcting P-values in multiple tests that promise to deliver increased power without greatly increasing the incidence of false-positive discovery.  Many of these methods involve resampling-based approaches such as the bootstrap (see, for example, [this article](https://www.econ.uzh.ch/dam/jcr:ffffffff-935a-b0d6-ffff-ffffd823d949/jasa.pdf) by Romano and Wolf).

::: {.column-margin}

The prevalence of the bootstrap in computationally-intensive mthods for correcting P-values in multiple tests might have had something to do with the reviewer's suggestion to use the bootstrap, but this is doubtful because the reviewer also wanted you to jettison P-values and make confidence intervals instead.  Anyhow, I want to make it clear that the bootstrap regressions performed by the `lmboot::wild_boot()` function do *not* address multiple tests.

:::

At this point, though, one has to pause and consider whether a decision to look for a more powerful method AFTER seeing the data---and even doing quite a bit of analysis on it---would be an instance of the very data-snooping practices that have contributed to the replication crisis in the social sciences and in other fields where researchers do not have means to directly observe the working of the natural processes that are involved in the generation of data and are therefore largely confined to searching for patterns in the data itself.
